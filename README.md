# ğŸ¤– Gemini Multi-Modal Chat Application

A sophisticated **multi-modal chat application** built with **Streamlit** and **FastAPI**, powered by **Google's Gemini AI model - gemini-2.5-flash**. This application supports intelligent conversations across **text**, **images**, and **CSV data analysis** â€” all in one seamless interface.

## âœ¨ Features

### ğŸ’¬ Core Chat Mode
- Multi-turn conversational chat with persistent memory
- Real-time streaming responses for natural conversation flow
- Markdown rendering support for rich text formatting
- Session-based chat history management

### ğŸ–¼ï¸ Image Chat Mode
- Upload and analyze images using Gemini's vision capabilities
- Ask natural language questions about image content
- Supports `.jpg`, `.jpeg`, `.png` formats
- Secure base64 encoding for backend transfer

### ğŸ“Š CSV Chat Mode
- Upload CSV files or import from URLs
- Intelligent data analysis with natural language queries
- Automatic data preview and summary statistics
- Interactive plotting and visualization support
- Context-aware data exploration powered by Gemini AI

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites
- Python **3.9.8**
- A valid **Google Gemini API key** ([Get one here](https://makersuite.google.com/app/apikey))
- Supabase to store chat histories.

### âš™ï¸ Installation

#### 1ï¸âƒ£ Clone the repository
```bash
git clone <your-repo-url>
cd Chatbot
```

#### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv .venv

# Activate it
# macOS / Linux
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

#### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```
or
```bash
pip install fastapi langchain google-generativeai plotly langchain_google_genai streamlit dotenv supabase langchain_experimental uvicorn
```
#### 4ï¸âƒ£ Set up environment variables
Create a `.env` file in the project root and add your Gemini API key:
```bash
GOOGLE_API_KEY=your_gemini_api_key_here
```

## â–¶ï¸ Running the Application

### ğŸ¨ Frontend (Streamlit)
```bash
cd frontend
streamlit main.py
```

### âš¡ Backend (FastAPI)
```bash
cd backend
py main.py
```

Once both services are running:
- **Frontend**: Open your browser at [http://localhost:8501](http://localhost:8501)
- **Backend API**: Available at [http://localhost:8000](http://localhost:8000)
- **API Documentation**: Visit [http://localhost:8000/docs](http://localhost:8000/docs) for interactive API docs

## ğŸ“ Project Structure

```
Chatbot/
â”œâ”€â”€ frontend/                    # Streamlit frontend application
â”‚   â”œâ”€â”€ main.py                 # Main Streamlit app entry point
â”‚   â”œâ”€â”€ core_chat.py            # Core text chat functionality
â”‚   â”œâ”€â”€ image_chat.py           # Image analysis chat mode
â”‚   â”œâ”€â”€ csv_chat.py             # CSV data analysis mode
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ sidebar.py          # Sidebar navigation component
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ api_client.py       # FastAPI backend communication
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ session_manager.py  # Chat session handling
â”‚       â”œâ”€â”€ database_setup.py   # Database initialization
â”‚       â””â”€â”€ helpers.py          # Utility functions
â”‚   
â”‚   
â”‚
â”œâ”€â”€ backend/                     # FastAPI backend application
â”‚   â”œâ”€â”€ main.py                 # FastAPI app entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ ai.py               # AI chat endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py             # Chat management endpoints
â”‚   â”‚   â””â”€â”€ sessions.py         # Session management endpoints
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ chat_service.py     # Chat business logic
â”‚   â”‚   â”œâ”€â”€ chat_repository.py  # Chat data persistence
â”‚   â”‚   â””â”€â”€ session_service.py  # Session management logic
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic data schemas
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ supabase_client.py  # Database client configuration
â”‚
â”œâ”€â”€ data/                        # Data storage directory
â”‚   â””â”€â”€ chats/                  # Chat session data files
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ chainlit.md                 # Chainlit welcome screen
â””â”€â”€ README.md                   # This file
```

## ğŸ”Œ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/ai/chat` | POST | Stream text-based chat responses |
| `/api/v1/ai/chat/image` | POST | Stream image analysis responses |
| `/api/v1/ai/chat/csv` | POST | CSV data analysis with plotting support |
| `/api/v1/sessions/{session_id}/messages` | GET | Retrieve messages from a session |
| `/api/v1/sessions/{session_id}/messages` | POST | Add a new message to a session |
| `/health` | GET | Health check endpoint |

## ğŸ—ï¸ Architecture

### Frontend (Streamlit)
- **UI Rendering**: Clean, responsive interface with mode switching
- **File Handling**: Upload and processing of images and CSV files
- **Session Management**: Persistent chat sessions with history
- **Real-time Communication**: Streaming responses from backend

### Backend (FastAPI)
- **API Gateway**: RESTful endpoints with automatic documentation
- **AI Integration**: Seamless integration with Google Gemini models
- **Data Processing**: CSV analysis with pandas and plotly visualization
- **CORS Support**: Cross-origin requests for frontend communication

### AI Service (Gemini)
- **Text Processing**: Natural language understanding and generation
- **Vision Analysis**: Image content analysis and description
- **Data Analysis**: Intelligent CSV data exploration and insights
- **Streaming Responses**: Real-time response generation

## ğŸ› ï¸ Technology Stack

- **Frontend**: Streamlit, Python
- **Backend**: FastAPI, Uvicorn
- **AI/ML**: Google Gemini API, LangChain
- **Data Processing**: Pandas, Plotly
- **Database**: Supabase (optional)

## ğŸ”§ Configuration

### Environment Variables
- `GOOGLE_API_KEY`: Your Google Gemini API key
- `SUPABASE_URL`: Supabase project URL
- `SUPABASE_KEY`: Supabase API key

### Model Configuration
The application uses `gemini-2.5-flash` by default. You can modify the model in `backend/services/ai_service.py`:
```python
def __init__(self, model_name="gemini-2.5-flash", temperature=0.3):
```
